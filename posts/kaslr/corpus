code_1_s OMIT
struct mem_vector {
    unsigned long start;
    unsigned long size;
};
code_1_e OMIT

code_2_s OMIT
enum mem_avoid_index {
    MEM_AVOID_ZO_RANGE = 0,
    MEM_AVOID_INITRD,
    MEM_AVOID_CMDLINE,
    MEM_AVOID_BOOTPARAMS,
    MEM_AVOID_MAX,
};

static struct mem_vector mem_avoid[MEM_AVOID_MAX];
code_2_e OMIT

code_3_s OMIT
mem_avoid[MEM_AVOID_ZO_RANGE].start = input;
mem_avoid[MEM_AVOID_ZO_RANGE].size = (output + init_size) - input;
code_3_e OMIT

code_4_s OMIT
initrd_start  = (u64)boot_params->ext_ramdisk_image << 32;
initrd_start |= boot_params->hdr.ramdisk_image;
initrd_size  = (u64)boot_params->ext_ramdisk_size << 32;
initrd_size |= boot_params->hdr.ramdisk_size;
mem_avoid[MEM_AVOID_INITRD].start = initrd_start;
mem_avoid[MEM_AVOID_INITRD].size = initrd_size;
code_4_e OMIT

code_5_s OMIT
cmd_line  = (u64)boot_params->ext_cmd_line_ptr << 32;
cmd_line |= boot_params->hdr.cmd_line_ptr;
/* Calculate size of cmd_line. */
ptr = (char *)(unsigned long)cmd_line;
for (cmd_line_size = 0; ptr[cmd_line_size++]; )
    ;
mem_avoid[MEM_AVOID_CMDLINE].start = cmd_line;
mem_avoid[MEM_AVOID_CMDLINE].size = cmd_line_size;
code_5_e OMIT

code_6_s OMIT
mem_avoid[MEM_AVOID_BOOTPARAMS].start = (unsigned long)boot_params;
mem_avoid[MEM_AVOID_BOOTPARAMS].size = sizeof(*boot_params);
code_6_e OMIT

code_7_s OMIT
/* Verify potential e820 positions, appending to slots list. */
for (i = 0; i < boot_params->e820_entries; i++) {
    process_e820_entry(&boot_params->e820_map[i], minimum,
               image_size);
    ...
}
code_7_e OMIT

code_8_s OMIT
/* Skip non-RAM entries. */
if (entry->type != E820_RAM)
    return;
code_8_e OMIT

code_9_s OMIT
#define KERNEL_IMAGE_SIZE	(512 * 1024 * 1024)

/* On 32-bit, ignore entries entirely above our maximum. */
if (IS_ENABLED(CONFIG_X86_32) && entry->addr >= KERNEL_IMAGE_SIZE)
    return;
code_9_e OMIT

code_10_s OMIT
/* Ignore entries entirely below our minimum. */
if (entry->addr + entry->size < minimum)
    return;
code_10_e OMIT

code_11_s OMIT
/* Potentially raise address to meet alignment needs. */
region.start = ALIGN(region.start, CONFIG_PHYSICAL_ALIGN);

/* Did we raise the address above this e820 region? */
if (region.start > entry->addr + entry->size)
    return;

/* Reduce size by any delta from the original address. */
region.size -= region.start - start_orig;

/* On 32-bit, reduce region size to fit within max size. */
if (IS_ENABLED(CONFIG_X86_32) &&
    region.start + region.size > KERNEL_IMAGE_SIZE)
    region.size = KERNEL_IMAGE_SIZE - region.start;

/* Return if region can't contain decompressed kernel */
if (region.size < image_size)
    return;
code_11_e OMIT

code_12_s OMIT
/* If nothing overlaps, store the region and return. */
if (!mem_avoid_overlap(&region, &overlap)) {
    store_slot_info(&region, image_size);
    return;
}
code_12_e OMIT

code_13_s OMIT
/* Store beginning of region if holds at least image_size. */
if (overlap.start > region.start + image_size) {
    struct mem_vector beginning;

    beginning.start = region.start;
    beginning.size = overlap.start - region.start;
    store_slot_info(&beginning, image_size);
}
code_13_e OMIT

code_14_s OMIT
/* Return if overlap extends to or past end of region. */
if (overlap.start + overlap.size >= region.start + region.size)
    return;

/* Clip off the overlapping region and start over. */
region.size -= overlap.start - region.start + overlap.size;
region.start = overlap.start + overlap.size;
code_14_e OMIT

code_15_s OMIT
#define MAX_SLOT_AREA 100
static struct slot_area slot_areas[MAX_SLOT_AREA];

static void store_slot_info(struct mem_vector *region, unsigned long image_size)
{
    ...
	slot_area.addr = region->start;
	slot_area.num = (region->size - image_size) /
			CONFIG_PHYSICAL_ALIGN + 1;
    ...
}
code_15_e OMIT

code_16_s OMIT
static unsigned long slots_fetch_random(void)
{
    ...
	slot = kaslr_get_random_long("Physical") % slot_max;

	for (i = 0; i < slot_area_index; i++) {
		if (slot >= slot_areas[i].num) {
			slot -= slot_areas[i].num;
			continue;
		}
		return slot_areas[i].addr + slot * CONFIG_PHYSICAL_ALIGN;
	}

	if (i == slot_area_index)
		debug_putstr("slots_fetch_random() failed!?\n");
	return 0;
}
code_16_e OMIT

code_17_s OMIT
void add_identity_map(unsigned long start, unsigned long size)
{
	/* Build the mapping. */
	kernel_ident_mapping_init(&mapping_info, (pgd_t *)level4p,
				  start, end);
}
int kernel_ident_mapping_init(struct x86_mapping_info *info, pgd_t *pgd_page,
			      unsigned long pstart, unsigned long pend)
{
	for (; addr < end; addr = next) {
		pgd_t *pgd = pgd_page + pgd_index(addr);
        ...
		if (pgd_present(*pgd)) {
			pud = pud_offset(pgd, 0);
			result = ident_pud_init(info, pud, addr, next);
			...
			continue;
		}
		pud = (pud_t *)info->alloc_pgt_page(info->context);
		...
		result = ident_pud_init(info, pud, addr, next);
		...
		set_pgd(pgd, __pgd(__pa(pud) | _KERNPG_TABLE));
	}
    ...
}
static int ident_pud_init(struct x86_mapping_info *info, pud_t *pud_page,
			  unsigned long addr, unsigned long end)
{
	unsigned long next;

	for (; addr < end; addr = next) {
		pud_t *pud = pud_page + pud_index(addr);
		if (pud_present(*pud)) {
			pmd = pmd_offset(pud, 0);
			ident_pmd_init(info, pmd, addr, next);
			continue;
		}
		pmd = (pmd_t *)info->alloc_pgt_page(info->context);
		ident_pmd_init(info, pmd, addr, next);
		set_pud(pud, __pud(__pa(pmd) | _KERNPG_TABLE));
	}
}
static void ident_pmd_init(struct x86_mapping_info *info, pmd_t *pmd_page,
			   unsigned long addr, unsigned long end)
{
	addr &= PMD_MASK;
	for (; addr < end; addr += PMD_SIZE) {
		pmd_t *pmd = pmd_page + pmd_index(addr);

		if (pmd_present(*pmd))
			continue;

		set_pmd(pmd, __pmd((addr - info->offset) | info->pmd_flag));
	}
}
code_17_e OMIT

code_18_s OMIT
void initialize_identity_maps(void)
{
	/* Init mapping_info with run-time function/buffer pointers. */
	mapping_info.alloc_pgt_page = alloc_pgt_page;
	mapping_info.context = &pgt_data;
    ...
}
static void *alloc_pgt_page(void *context)
{
    ...
	entry = pages->pgt_buf + pages->pgt_buf_offset;
	pages->pgt_buf_offset += PAGE_SIZE;

	return entry;
}
code_18_e OMIT

code_19_s OMIT
void initialize_identity_maps(void)
{
    ...
	pgt_data.pgt_buf_offset = 0;
	/*
	 * If we came here via startup_32(), cr3 will be _pgtable already
	 * and we must append to the existing area instead of entirely
	 * overwriting it.
	 */
	level4p = read_cr3();
	if (level4p == (unsigned long)_pgtable) {
		debug_putstr("booted via startup_32()\n");
		pgt_data.pgt_buf = _pgtable + BOOT_INIT_PGT_SIZE;
		pgt_data.pgt_buf_size = BOOT_PGT_SIZE - BOOT_INIT_PGT_SIZE;
		memset(pgt_data.pgt_buf, 0, pgt_data.pgt_buf_size);
	} else {
		debug_putstr("booted via startup_64()\n");
		pgt_data.pgt_buf = _pgtable;
		pgt_data.pgt_buf_size = BOOT_PGT_SIZE;
		memset(pgt_data.pgt_buf, 0, pgt_data.pgt_buf_size);
		level4p = (unsigned long)alloc_pgt_page(&pgt_data);
	}
}
code_19_e OMIT

code_20_s OMIT
// vmlinux.lds.S
       .pgtable : {
		_pgtable = . ;
		*(.pgtable)
		_epgtable = . ;
	}

// arch/x86/kernel/head_64.S
	.section ".pgtable","a",@nobits
	.balign 4096
pgtable:
	.fill BOOT_PGT_SIZE, 1, 0

# define BOOT_INIT_PGT_SIZE	(6*4096)
#  ifdef CONFIG_X86_VERBOSE_BOOTUP
#   define BOOT_PGT_SIZE	(19*4096)
#  else /* !CONFIG_X86_VERBOSE_BOOTUP */
#   define BOOT_PGT_SIZE	(17*4096)
#  endif
code_20_e OMIT

code_21_s OMIT
static __initdata struct kaslr_memory_region {
    unsigned long *base;
    unsigned long size_tb;
} kaslr_regions[] = {
    { &page_offset_base, 64/* Maximum */ },
    { &vmalloc_base, VMALLOC_SIZE_TB },
    { &vmemmap_base, 1 },
};
code_21_e OMIT

code_22_s OMIT
void __init kernel_randomize_memory(void)
{
    ...
	for (i = 0; i < ARRAY_SIZE(kaslr_regions); i++) {
		unsigned long entropy;

		/*
		 * Select a random virtual address using the extra entropy
		 * available.
		 */
		entropy = remain_entropy / (ARRAY_SIZE(kaslr_regions) - i);
		prandom_bytes_state(&rand_state, &rand, sizeof(rand));
		entropy = (rand % (entropy + 1)) & PUD_MASK;
		vaddr += entropy;
		*kaslr_regions[i].base = vaddr;

		/*
		 * Jump the region and add a minimum padding based on
		 * randomization alignment.
		 */
		vaddr += get_padding(&kaslr_regions[i]);
		vaddr = round_up(vaddr + 1, PUD_SIZE);
		remain_entropy -= entropy;
	}
}
code_22_e OMIT
